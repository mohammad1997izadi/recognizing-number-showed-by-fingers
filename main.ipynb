{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a3fdad",
   "metadata": {},
   "source": [
    "# Classification Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657ee93",
   "metadata": {},
   "source": [
    "\n",
    "You will build a Neural Network for classification . You can use keras or tensorflow or pytorch in this assignmen. first import dataset by running cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fc368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec4c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_img(folder, number):\n",
    "    set_x_list = []\n",
    "    set_y_list = []\n",
    "\n",
    "    num_px = 64\n",
    "    classes = (\"0\", \"1\",\"2\", \"3\",\"4\",\"5\")\n",
    "    for i in range(number):\n",
    "        zero = \"./\"+str(folder)+\"/0/IMG (\"+str(i)+\").jpg\"\n",
    "        one = \"./\"+str(folder)+\"/1/IMG (\"+str(i)+\").jpg\"\n",
    "        two  =\"./\"+str(folder)+\"/2/IMG (\"+str(i)+\").jpg\"\n",
    "        three =\"./\"+str(folder)+\"/3/IMG (\"+str(i)+\").jpg\"\n",
    "        four =\"./\"+str(folder)+\"/4/IMG (\"+str(i)+\").jpg\"\n",
    "        five =\"./\"+str(folder)+\"/5/IMG (\"+str(i)+\").jpg\"\n",
    "\n",
    "        zero_full_image = imread(zero)\n",
    "        zero_image = resize(zero_full_image, (num_px,num_px), anti_aliasing=True, mode='reflect')\n",
    "        set_x_list.append(zero_image)\n",
    "        set_y_list.append(0)\n",
    "\n",
    "        one_full_image = imread(one)\n",
    "        one_image = resize(one_full_image, (num_px,num_px), anti_aliasing=True, mode='reflect')\n",
    "        set_x_list.append(one_image)\n",
    "        set_y_list.append(1)\n",
    "\n",
    "        two_full_image = imread(two)\n",
    "        two_image = resize(two_full_image, (num_px,num_px), anti_aliasing=True, mode='reflect')\n",
    "        set_x_list.append(two_image)\n",
    "        set_y_list.append(2)\n",
    "\n",
    "        three_full_image = imread(three)\n",
    "        three_image = resize(three_full_image, (num_px,num_px), anti_aliasing=True, mode='reflect')\n",
    "        set_x_list.append(three_image)\n",
    "        set_y_list.append(3)\n",
    "\n",
    "        four_full_image = imread(four)\n",
    "        four_image = resize(four_full_image, (num_px,num_px), anti_aliasing=True, mode='reflect')\n",
    "        set_x_list.append(four_image)\n",
    "        set_y_list.append(4)\n",
    "\n",
    "        five_full_image = imread(five)\n",
    "        five_image = resize(five_full_image, (num_px,num_px), anti_aliasing=True, mode='reflect')\n",
    "        set_x_list.append(five_image)\n",
    "        set_y_list.append(5)\n",
    "        \n",
    "\n",
    "    set_x = np.array(set_x_list)\n",
    "    set_y = np.array(set_y_list)\n",
    "\n",
    "    num_pixels = set_x.shape[1] * set_x.shape[2] *set_x.shape[3]\n",
    "    set_x =set_x.reshape(set_x.shape[0], set_x.shape[3], set_x.shape[1], set_x.shape[2] ).astype('float32')\n",
    "\n",
    "    num_classes = 6\n",
    "    set_y = to_categorical(set_y, num_classes)\n",
    "\n",
    "    return set_x, set_y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b1ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_x, set_y = read_img('dataset', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f48fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(set_x,set_y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append([X_train[i], y_train[i]])\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append([X_test[i], y_test[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934eac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "\n",
    "# regular model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel,self).__init__()\n",
    "\n",
    "        #conv 1\n",
    "        self.cnn1=nn.Conv2d(in_channels=3, out_channels=3, kernel_size=2, stride=1, padding=2)\n",
    "        self.relu1=nn.ReLU()\n",
    "\n",
    "        #Maxpool 1\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        #conv 2\n",
    "        self.cnn2=nn.Conv2d(in_channels=3, out_channels=12, kernel_size=2, stride=1, padding=4)\n",
    "        self.relu2=nn.ReLU()\n",
    "\n",
    "        \n",
    "        #dropout 1\n",
    "        self.d1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        \n",
    "        #Maxpool 2\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        #FC 1\n",
    "        self.fc1=nn.Linear(4800,1500)\n",
    "        \n",
    "        \n",
    "        #dropout 2\n",
    "        self.d2 = nn.Dropout(p=0.25)\n",
    "\n",
    "        \n",
    "        #FC 2\n",
    "        self.fc2=nn.Linear(1500,6)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "       \n",
    "        #conv1\n",
    "        out=self.cnn1(x)\n",
    "        out=self.relu1(out)\n",
    "        \n",
    "        #max poo1 1\n",
    "        out=self.maxpool1(out)\n",
    "        \n",
    "        #conv2\n",
    "        out=self.cnn2(out)\n",
    "        out=self.relu2(out)\n",
    "       \n",
    "        #dropout 1\n",
    "        out=self.d1(out)\n",
    "        \n",
    "        #max poo1 2\n",
    "        out=self.maxpool2(out)\n",
    "        \n",
    "        out=out.view(out.size(0),-1)\n",
    "#         print(out.shape)\n",
    "        #fc1\n",
    "        out=self.fc1(out)\n",
    "        \n",
    "        #dropout 2\n",
    "        out=self.d2(out)\n",
    "        \n",
    "        #fc2\n",
    "        out=self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# overfit model\n",
    "class CNNModel_over(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel_over,self).__init__()\n",
    "\n",
    "        #conv 1\n",
    "        self.cnn1=nn.Conv2d(in_channels=3, out_channels=3, kernel_size=2, stride=1, padding=2)\n",
    "        self.relu1=nn.ReLU()\n",
    "\n",
    "        #Maxpool 1\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        #conv 2\n",
    "        self.cnn2=nn.Conv2d(in_channels=3, out_channels=32, kernel_size=2, stride=1, padding=4)\n",
    "        self.relu2=nn.ReLU()\n",
    "\n",
    "        #FC 1\n",
    "        self.fc1=nn.Linear(51200,3000)\n",
    "\n",
    "        \n",
    "        #FC 2\n",
    "        self.fc2=nn.Linear(3000,6)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "       \n",
    "        #conv1\n",
    "        out=self.cnn1(x)\n",
    "        out=self.relu1(out)\n",
    "        \n",
    "        #max poo1 1\n",
    "        out=self.maxpool1(out)\n",
    "        \n",
    "        #conv2\n",
    "        out=self.cnn2(out)\n",
    "        out=self.relu2(out)\n",
    "       \n",
    "    \n",
    "        \n",
    "        out=out.view(out.size(0),-1)\n",
    "#         print(out.shape)\n",
    "        \n",
    "        #fc1\n",
    "        out=self.fc1(out)\n",
    "\n",
    "        #fc2\n",
    "        out=self.fc2(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "   \n",
    "# underfit model\n",
    "class CNNModel_under(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel_under,self).__init__()\n",
    "\n",
    "        #conv 1\n",
    "        self.cnn1=nn.Conv2d(in_channels=3, out_channels=1, kernel_size=6, stride=6, padding=0)\n",
    "        self.relu1=nn.ReLU()\n",
    "        \n",
    "        #FC 1\n",
    "        self.fc1=nn.Linear(100,6)\n",
    "\n",
    "    def forward(self,x):\n",
    "       \n",
    "        #conv1\n",
    "        out=self.cnn1(x)\n",
    "        out=self.relu1(out)\n",
    "        \n",
    "        out=out.view(out.size(0),-1)\n",
    "#         print(out.shape)\n",
    "        #fc1\n",
    "        out=self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fa6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prob, label):\n",
    "    prob = prob.bool()\n",
    "    label = label.bool()\n",
    "    epsilon = 1e-7\n",
    "    TP = (prob & label).sum().float()\n",
    "    TN = ((~prob) & (~label)).sum().float()\n",
    "    FP = (prob & (~label)).sum().float()\n",
    "    FN = ((~prob) & label).sum().float()\n",
    "    #accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    precision = torch.mean(TP / (TP + FP + epsilon))\n",
    "    recall = torch.mean(TP / (TP + FN + epsilon))\n",
    "    F2 = 2 * precision * recall / (precision + recall + epsilon)\n",
    "    return precision, recall, F2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7fc9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===================== underfit model ===================== \n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.795169711112976. Accuracy18.75 \n",
      "Epoch : 5\n",
      "trainset) Iteration 46. Loss 1.7932716608047485. Accuracy18.75 \n",
      "Epoch : 10\n",
      "trainset) Iteration 91. Loss 1.7900522947311401. Accuracy18.75 \n",
      "Epoch : 15\n",
      "trainset) Iteration 136. Loss 1.7913408279418945. Accuracy15.625 \n",
      "testset) Iteration 136. Loss 1.7913408279418945. Accuracy17.5 . precission0.8583333492279053 . recall1.0 . f1_score0.9237667918205261\n",
      " ===================== overfit model ===================== \n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.7924444675445557. Accuracy12.5 \n",
      "Epoch : 5\n",
      "trainset) Iteration 676. Loss 0.39915090799331665. Accuracy87.5 \n",
      "Epoch : 10\n",
      "trainset) Iteration 1351. Loss 0.1436077505350113. Accuracy100.0 \n",
      "Epoch : 15\n",
      "trainset) Iteration 2026. Loss 0.00039965243195183575. Accuracy100.0 \n",
      "testset) Iteration 2026. Loss 0.00039965243195183575. Accuracy80.0 . precission0.9750000238418579 . recall0.9698412418365479 . f1_score0.9697147607803345\n",
      "===================== normal model - learning rate0.001 ===================== \n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.7936426401138306. Accuracy10.9375 \n",
      "Epoch : 5\n",
      "trainset) Iteration 86. Loss 1.0144977569580078. Accuracy59.375 \n",
      "Epoch : 10\n",
      "trainset) Iteration 171. Loss 0.40554335713386536. Accuracy84.375 \n",
      "Epoch : 15\n",
      "trainset) Iteration 256. Loss 0.37249279022216797. Accuracy84.375 \n",
      "testset) Iteration 256. Loss 0.37249279022216797. Accuracy80.83333587646484 . precission0.97969651222229 . recall0.9414772987365723 . f1_score0.9602048397064209\n",
      "=========================== more 4 different learning rates ===========================\n",
      "===========================normal model - learning rate 1 - mini batch: 64 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.791825294494629. Accuracy20.3125 \n",
      "Epoch : 5\n",
      "trainset) Iteration 86. Loss 1128.57275390625. Accuracy7.8125 \n",
      "Epoch : 10\n",
      "trainset) Iteration 171. Loss 919.7150268554688. Accuracy10.9375 \n",
      "Epoch : 15\n",
      "trainset) Iteration 256. Loss 546.8486328125. Accuracy15.625 \n",
      "testset) Iteration 256. Loss 546.8486328125. Accuracy16.66666603088379 . precission0.8571428060531616 . recall0.747358500957489 . f1_score0.7982134222984314\n",
      "===========================normal model - learning rate 0.1 - mini batch: 64 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.811495304107666. Accuracy10.9375 \n",
      "Epoch : 5\n",
      "trainset) Iteration 86. Loss 3.3250489234924316. Accuracy14.0625 \n",
      "Epoch : 10\n",
      "trainset) Iteration 171. Loss 4.3766655921936035. Accuracy14.0625 \n",
      "Epoch : 15\n",
      "trainset) Iteration 256. Loss 2.2467141151428223. Accuracy18.75 \n",
      "testset) Iteration 256. Loss 2.2467141151428223. Accuracy15.0 . precission0.8581242561340332 . recall0.9611613750457764 . f1_score0.9053263664245605\n",
      "===========================normal model - learning rate 0.01 - mini batch: 64 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.7980520725250244. Accuracy17.1875 \n",
      "Epoch : 5\n",
      "trainset) Iteration 86. Loss 1.7918908596038818. Accuracy21.875 \n",
      "Epoch : 10\n",
      "trainset) Iteration 171. Loss 1.7851026058197021. Accuracy14.0625 \n",
      "Epoch : 15\n",
      "trainset) Iteration 256. Loss 1.7914977073669434. Accuracy17.1875 \n",
      "testset) Iteration 256. Loss 1.7914977073669434. Accuracy14.166666984558105 . precission0.8562091588973999 . recall0.29672005772590637 . f1_score0.4391891360282898\n",
      "===========================normal model - learning rate 0.0001 - mini batch: 64 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.788115382194519. Accuracy18.75 \n",
      "Epoch : 5\n",
      "trainset) Iteration 86. Loss 1.5541847944259644. Accuracy57.8125 \n",
      "Epoch : 10\n",
      "trainset) Iteration 171. Loss 1.2181798219680786. Accuracy54.6875 \n",
      "Epoch : 15\n",
      "trainset) Iteration 256. Loss 1.0320712327957153. Accuracy71.875 \n",
      "testset) Iteration 256. Loss 1.0320712327957153. Accuracy65.0 . precission0.9703704118728638 . recall0.9323707818984985 . f1_score0.9509880542755127\n",
      "=========================== different batch sizes ===========================\n",
      "=========================== normal model - learning rate: 0.001 - mini batch: 8 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.9204503297805786. Accuracy0.0 \n",
      "Epoch : 5\n",
      "trainset) Iteration 676. Loss 0.4385376572608948. Accuracy87.5 \n",
      "Epoch : 10\n",
      "trainset) Iteration 1351. Loss 0.2908259630203247. Accuracy87.5 \n",
      "Epoch : 15\n",
      "trainset) Iteration 2026. Loss 0.03274568170309067. Accuracy100.0 \n",
      "testset) Iteration 2026. Loss 0.03274568170309067. Accuracy75.0 . precission0.977142870426178 . recall0.9226190447807312 . f1_score0.947772204875946\n",
      "=========================== normal model - learning rate: 0.001 - mini batch: 16 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.7673523426055908. Accuracy12.5 \n",
      "Epoch : 5\n",
      "trainset) Iteration 341. Loss 0.4723246395587921. Accuracy75.0 \n",
      "Epoch : 10\n",
      "trainset) Iteration 681. Loss 0.14847180247306824. Accuracy93.75 \n",
      "Epoch : 15\n",
      "trainset) Iteration 1021. Loss 0.11240143328905106. Accuracy93.75 \n",
      "testset) Iteration 1021. Loss 0.11240143328905106. Accuracy78.33333587646484 . precission0.9455357193946838 . recall0.9903846383094788 . f1_score0.9644215106964111\n",
      "=========================== normal model - learning rate: 0.001 - mini batch: 32 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.764509677886963. Accuracy25.0 \n",
      "Epoch : 5\n",
      "trainset) Iteration 171. Loss 1.624275803565979. Accuracy34.375 \n",
      "Epoch : 10\n",
      "trainset) Iteration 341. Loss 0.9109649062156677. Accuracy62.5 \n",
      "Epoch : 15\n",
      "trainset) Iteration 511. Loss 0.49987414479255676. Accuracy81.25 \n",
      "testset) Iteration 511. Loss 0.49987414479255676. Accuracy60.0 . precission0.9362062811851501 . recall0.9820512533187866 . f1_score0.9585278034210205\n",
      "=========================== normal model - learning rate: 0.001 - mini batch: 64 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.7860959768295288. Accuracy15.625 \n",
      "Epoch : 5\n",
      "trainset) Iteration 86. Loss 1.5053201913833618. Accuracy32.8125 \n",
      "Epoch : 10\n",
      "trainset) Iteration 171. Loss 1.0477818250656128. Accuracy60.9375 \n",
      "Epoch : 15\n",
      "trainset) Iteration 256. Loss 0.5866484642028809. Accuracy82.8125 \n",
      "testset) Iteration 256. Loss 0.5866484642028809. Accuracy71.66666412353516 . precission0.9698535799980164 . recall0.9149425029754639 . f1_score0.9415583610534668\n",
      "=========================== normal model - learning rate: 0.001 - mini batch: 128 ===========================\n",
      "Epoch : 0\n",
      "trainset) Iteration 1. Loss 1.7936019897460938. Accuracy17.1875 \n",
      "Epoch : 5\n",
      "trainset) Iteration 46. Loss 1.8153516054153442. Accuracy22.65625 \n",
      "Epoch : 10\n",
      "trainset) Iteration 91. Loss 1.8426904678344727. Accuracy14.0625 \n",
      "Epoch : 15\n",
      "trainset) Iteration 136. Loss 1.6733156442642212. Accuracy31.25 \n",
      "testset) Iteration 136. Loss 1.6733156442642212. Accuracy32.5 . precission0.9878048896789551 . recall0.7864077687263489 . f1_score0.8756756782531738\n"
     ]
    }
   ],
   "source": [
    "# training section \n",
    "\n",
    "model_changer = 0\n",
    "epochs = 16\n",
    "learning_rate=0.001\n",
    "\n",
    "for model_changer in range(3):\n",
    "\n",
    "    if model_changer == 0:\n",
    "        print(' ===================== underfit model ===================== ')\n",
    "        \n",
    "        train_dataset = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True, num_workers=4)\n",
    "        test_dataset = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True, num_workers=4)\n",
    "        model=CNNModel_under()\n",
    "        \n",
    "    if model_changer == 1:\n",
    "        print(' ===================== overfit model ===================== ')\n",
    "        \n",
    "        train_dataset = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, num_workers=4)\n",
    "        test_dataset = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True, num_workers=4)\n",
    "        model=CNNModel_over()\n",
    "\n",
    "    if model_changer == 2:\n",
    "        print('===================== normal model - learning rate' + str(learning_rate)+' ===================== ')\n",
    "        \n",
    "        train_dataset = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "        test_dataset = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "        model=CNNModel()\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    iter=0\n",
    "    for epoch in range(epochs):\n",
    "        flag1 = False\n",
    "        flag2 = False\n",
    "        for i,(images,labels) in enumerate(train_dataset):\n",
    "\n",
    "            images=Variable(images)\n",
    "            labels=Variable(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs=model(images)\n",
    "\n",
    "            labels = labels.softmax(dim=1)\n",
    "            loss=criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            iter+=1\n",
    "\n",
    "            if epoch%5 == 0 and flag1 == False:\n",
    "                print('Epoch : '+ str(epoch))\n",
    "                flag1 = True\n",
    "                #trainset accuracy\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted == torch.max(labels, 1)[1]).sum()\n",
    "                accuracy = 100*correct/total\n",
    "                print('trainset) Iteration {}. Loss {}. Accuracy{} '.format(iter,loss.data.item(),accuracy))\n",
    "\n",
    "            if epoch == 15 and flag2 == False:\n",
    "                flag2 = True\n",
    "\n",
    "                #validation set report\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                counter = 0\n",
    "                total_f1 = 0\n",
    "                total_recall = 0\n",
    "                total_precission = 0\n",
    "\n",
    "                for images,labels in test_dataset:\n",
    "\n",
    "                    counter +=1\n",
    "                    images=Variable(images)\n",
    "\n",
    "                    outputs=model(images)\n",
    "\n",
    "                    _,predicted=torch.max(outputs.data,1)\n",
    "\n",
    "                    total+=labels.size(0)\n",
    "\n",
    "                    correct+=(predicted == torch.max(labels, 1)[1]).sum()\n",
    "\n",
    "                    precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "                    total_precission += precission\n",
    "                    total_recall += recall\n",
    "                    total_f1 += f1\n",
    "\n",
    "\n",
    "\n",
    "                total_f1 = total_f1 / counter\n",
    "                total_recall = total_recall / counter\n",
    "                total_precission = total_precission / counter\n",
    "\n",
    "                precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "                accuracy = 100*correct/total\n",
    "                print('testset) Iteration {}. Loss {}. Accuracy{} . precission{} . recall{} . f1_score{}'.format(iter,loss.data.item(),accuracy, total_precission, total_recall, total_f1))\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_dataset = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "learning_rate_changer = 0\n",
    "print('=========================== more 4 different learning rates ===========================')\n",
    "for learning_rate in [1, 0.1, 0.01, 0.0001]:\n",
    "\n",
    "    print('===========================normal model - learning rate ' + str(learning_rate) + ' - mini batch: 64 ===========================') \n",
    "    model=CNNModel()\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    iter=0\n",
    "    for epoch in range(epochs):\n",
    "        flag1 = False\n",
    "        flag2 = False\n",
    "        for i,(images,labels) in enumerate(train_dataset):\n",
    "\n",
    "            images=Variable(images)\n",
    "            labels=Variable(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs=model(images)\n",
    "\n",
    "            labels = labels.softmax(dim=1)\n",
    "            loss=criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            iter+=1\n",
    "\n",
    "            if epoch%5 == 0 and flag1 == False:\n",
    "                print('Epoch : '+ str(epoch))\n",
    "                flag1 = True\n",
    "                #trainset accuracy\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted == torch.max(labels, 1)[1]).sum()\n",
    "                accuracy = 100*correct/total\n",
    "                print('trainset) Iteration {}. Loss {}. Accuracy{} '.format(iter,loss.data.item(),accuracy))\n",
    "\n",
    "            if epoch == 15 and flag2 == False:\n",
    "                flag2 = True\n",
    "\n",
    "                #validation set report\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                counter = 0\n",
    "                total_f1 = 0\n",
    "                total_recall = 0\n",
    "                total_precission = 0\n",
    "\n",
    "                for images,labels in test_dataset:\n",
    "\n",
    "                    counter +=1\n",
    "                    images=Variable(images)\n",
    "\n",
    "                    outputs=model(images)\n",
    "\n",
    "                    _,predicted=torch.max(outputs.data,1)\n",
    "\n",
    "                    total+=labels.size(0)\n",
    "\n",
    "                    correct+=(predicted == torch.max(labels, 1)[1]).sum()\n",
    "\n",
    "                    precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "                    total_precission += precission\n",
    "                    total_recall += recall\n",
    "                    total_f1 += f1\n",
    "\n",
    "\n",
    "\n",
    "                total_f1 = total_f1 / counter\n",
    "                total_recall = total_recall / counter\n",
    "                total_precission = total_precission / counter\n",
    "\n",
    "                precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "                accuracy = 100*correct/total\n",
    "                print('testset) Iteration {}. Loss {}. Accuracy{} . precission{} . recall{} . f1_score{}'.format(iter,loss.data.item(),accuracy, total_precission, total_recall, total_f1))\n",
    "           \n",
    "        \n",
    "        \n",
    "\n",
    "mini_batch = 0\n",
    "print('=========================== different batch sizes ===========================')\n",
    "\n",
    "for mini_batch in [8, 16, 32, 64, 128]:\n",
    "    \n",
    "    train_dataset = torch.utils.data.DataLoader(train_data, batch_size=mini_batch, shuffle=True, num_workers=4)\n",
    "    test_dataset = torch.utils.data.DataLoader(test_data, batch_size=mini_batch, shuffle=True, num_workers=4)\n",
    "    learning_rate=0.001\n",
    "    print('=========================== normal model - learning rate: ' + str(learning_rate) + ' - mini batch: '+ str(mini_batch)+' ===========================' ) \n",
    "    model=CNNModel()\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    iter=0\n",
    "    for epoch in range(epochs):\n",
    "        flag1 = False\n",
    "        flag2 = False\n",
    "        for i,(images,labels) in enumerate(train_dataset):\n",
    "\n",
    "            images=Variable(images)\n",
    "            labels=Variable(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs=model(images)\n",
    "\n",
    "            labels = labels.softmax(dim=1)\n",
    "            loss=criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            iter+=1\n",
    "\n",
    "            if epoch%5 == 0 and flag1 == False:\n",
    "                print('Epoch : '+ str(epoch))\n",
    "                flag1 = True\n",
    "                #trainset accuracy\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted == torch.max(labels, 1)[1]).sum()\n",
    "                accuracy = 100*correct/total\n",
    "                print('trainset) Iteration {}. Loss {}. Accuracy{} '.format(iter,loss.data.item(),accuracy))\n",
    "\n",
    "            if epoch == 15 and flag2 == False:\n",
    "                flag2 = True\n",
    "\n",
    "                #validation set report\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                counter = 0\n",
    "                total_f1 = 0\n",
    "                total_recall = 0\n",
    "                total_precission = 0\n",
    "\n",
    "                for images,labels in test_dataset:\n",
    "\n",
    "                    counter +=1\n",
    "                    images=Variable(images)\n",
    "\n",
    "                    outputs=model(images)\n",
    "\n",
    "                    _,predicted=torch.max(outputs.data,1)\n",
    "\n",
    "                    total+=labels.size(0)\n",
    "\n",
    "                    correct+=(predicted == torch.max(labels, 1)[1]).sum()\n",
    "\n",
    "                    precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "                    total_precission += precission\n",
    "                    total_recall += recall\n",
    "                    total_f1 += f1\n",
    "\n",
    "\n",
    "\n",
    "                total_f1 = total_f1 / counter\n",
    "                total_recall = total_recall / counter\n",
    "                total_precission = total_precission / counter\n",
    "\n",
    "                precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "                accuracy = 100*correct/total\n",
    "                print('testset) Iteration {}. Loss {}. Accuracy{} . precission{} . recall{} . f1_score{}'.format(iter,loss.data.item(),accuracy, total_precission, total_recall, total_f1))\n",
    "              \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d002e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== best normal model - learning rate:0.001 - batch size: - 64 =========================== \n",
      "testset) Iteration 571. Loss 0.03173132613301277. Accuracy22.22222137451172 . precission0.8999999761581421 . recall0.6000000238418579 . f1_score0.7199999690055847\n"
     ]
    }
   ],
   "source": [
    "set_x, set_y = read_img('dataset', 200)\n",
    "set_x_test, set_y_test = read_img('my_hand_dataset', 3)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(set_x)\n",
    "y_train = torch.tensor(set_y)\n",
    "\n",
    "X_test = torch.tensor(set_x_test)\n",
    "y_test = torch.tensor(set_y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append([X_train[i], y_train[i]])\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append([X_test[i], y_test[i]])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "learning_rate=0.001\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataset = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('=========================== best normal model - learning rate:' + str(learning_rate)+ ' - batch size: - '+str(batch_size) + ' =========================== ') \n",
    "model=CNNModel()\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "epochs = 31\n",
    "iter=0\n",
    "for epoch in range(epochs):\n",
    "    flag = False\n",
    "    for i,(images,labels) in enumerate(train_dataset):\n",
    "\n",
    "        images=Variable(images)\n",
    "        labels=Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs=model(images)\n",
    "\n",
    "        labels = labels.softmax(dim=1)\n",
    "        loss=criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        iter+=1\n",
    "\n",
    "        if epoch == 30 and flag == False:\n",
    "            flag = True\n",
    "\n",
    "            #validation set report\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            counter = 0\n",
    "            total_f1 = 0\n",
    "            total_recall = 0\n",
    "            total_precission = 0\n",
    "\n",
    "            for images,labels in test_dataset:\n",
    "\n",
    "                counter +=1\n",
    "                images=Variable(images)\n",
    "\n",
    "                outputs=model(images)\n",
    "\n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "\n",
    "                total+=labels.size(0)\n",
    "\n",
    "                correct+=(predicted == torch.max(labels, 1)[1]).sum()\n",
    "\n",
    "                precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "                total_precission += precission\n",
    "                total_recall += recall\n",
    "                total_f1 += f1\n",
    "\n",
    "\n",
    "\n",
    "            total_f1 = total_f1 / counter\n",
    "            total_recall = total_recall / counter\n",
    "            total_precission = total_precission / counter\n",
    "\n",
    "            precission, recall, f1 = F1_score(predicted, torch.max(labels, 1)[1])\n",
    "            accuracy = 100*correct/total\n",
    "            print('testset) Iteration {}. Loss {}. Accuracy{} . precission{} . recall{} . f1_score{}'.format(iter,loss.data.item(),accuracy, total_precission, total_recall, total_f1))\n",
    "\n",
    "\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
